# ğŸ—ï¸ On-Prem SQL to Azure Data Engineering Project

This project demonstrates how to modernize an on-premises SQL Server data platform by migrating to Azure, enabling scalable analytics, cost optimization, and advanced business insights. It follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** and implements end-to-end data engineering practices.

---

## ğŸ¯ Business Goal

The goal of this project is to modernize the existing on-prem SQL data platform by moving it to Azure, enabling scalable analytics, cost optimization, and advanced business insights.

### Key Objectives

- ğŸ“¥ **Reliable Data Ingestion** â€“ Automate data loads from on-prem SQL to Azure Data Lake.  
- â³ **Incremental Loading** â€“ Efficiently capture only changed records to reduce costs and improve performance.  
- âš¡ **Data Transformation** â€“ Standardize and process data in Databricks for analytics.  
- ğŸ“Š **Business Insights** â€“ Provide clean, aggregated datasets for reporting in Power BI.  
- ğŸ”” **Operational Monitoring** â€“ Enable automated email alerts for pipeline health.  
- ğŸš€ **CI/CD Deployment** â€“ Manage deployments with Azure DevOps and GitHub integration.  

---

## ğŸ—ï¸ Architecture & Tech Stack

![Architecture Diagram](https://github.com/user-attachments/assets/32e4e1eb-d892-44d1-83bc-a6c922db6487)  


**Architecture Flow:**  

- **Ingestion Layer (Bronze)** â€“ On-prem SQL data ingested using ADF into ADLS raw zone.  
- **Transformation Layer (Silver)** â€“ Databricks performs cleansing, standardization, and enrichment.  
- **Aggregation Layer (Gold)** â€“ Databricks creates curated Delta Tables for BI consumption.  
- **Consumption** â€“ Power BI dashboards built on Gold layer datasets.  
- **Monitoring & Deployment** â€“ Logic Apps for alerts, Azure DevOps for CI/CD.

**Tools & Services Used:**  

- ğŸ”„ Data Ingestion: Azure Data Factory (ForEach, If Condition, Incremental Loads)  
- ğŸ—„ï¸ Storage: Azure Data Lake Storage (Bronze/Silver/Gold)  
- ğŸ”¥ Processing: Azure Databricks (PySpark transformations)  
- ğŸ—ƒï¸ Format: Delta Lake (partitioning, schema evolution, time travel)  
- ğŸ”” Alerts: Logic Apps (email notifications on pipeline failure/success)  
- ğŸš€ CI/CD: Azure DevOps + GitHub (ARM template deployments)  
- ğŸ“Š Visualization: Power BI  

---

## ğŸ”„ Step-by-Step Implementation

### 1ï¸âƒ£ Data Ingestion (Bronze Layer)

- Built a **metadata-driven pipeline** in ADF using `ForEach` and `If Condition`.  
- Full load for tables without `LastModified` column (e.g., `DimAirport`).  
- Incremental load for tables with `LastModified` column (e.g., `DimFlight`, `DimPassenger`).  
- Implemented **HighWatermark logic** for incremental processing.  
- Data stored in ADLS Bronze folder (`/bronze/{TableName}/`).  

### 2ï¸âƒ£ Transformation (Silver Layer)

- Used Databricks notebooks (PySpark) to clean and standardize data.  
- Handled nulls, standardized datetime formats, applied schema enforcement.  
- Stored transformed data in **Parquet format** in the Silver zone.  

### 3ï¸âƒ£ Data Aggregation (Gold Layer)

- Created Delta Tables for business-ready datasets.  
- Implemented partitioning for performance.  
- Supported **time travel** for auditing and history tracking.  

### 4ï¸âƒ£ Monitoring with Logic Apps

- Configured Logic Apps to trigger on ADF pipeline success/failure.  
- Sent **email alerts** to stakeholders.  

### 5ï¸âƒ£ Deployment with Azure DevOps

- Exported **ADF ARM templates** to `adf_publish/` folder.  
- Configured **Azure DevOps pipeline** for CI/CD deployment.  
- Version-controlled all assets in GitHub (ADF pipelines, Databricks notebooks, Logic Apps).  

---

## ğŸ›‘ Challenges & Solutions

| âš ï¸ Challenge | âœ… Solution |
|--------------|------------|
| Duplicate activity names in ADF | Used dynamic names: `FullLoad_@{item().TableName}` / `IncrementalLoad_@{item().TableName}` |
| Misuse of `@item()` | Restricted usage inside ForEach child activities only |
| Initial watermark handling | Defaulted to `'1900-01-01 00:00:00.000'` for first load |
| Query performance issues | Applied Delta Lake optimizations (partitioning, caching) |
| Cost management | Enabled Databricks cluster auto-termination + Parquet compression |
| Schema evolution | Used Delta Lake features to handle schema changes automatically |

---

## ğŸ‘¨â€ğŸ’» Role & Contributions

- ğŸ—ï¸ Designed and implemented the **end-to-end data pipeline** in Azure.  
- ğŸ”„ Built **incremental + full load ingestion** logic using ADF.  
- ğŸ”¥ Performed data cleansing and standardization with PySpark in Databricks.  
- ğŸ—ƒï¸ Optimized storage with Parquet and Delta Lake.  
- ğŸ”” Automated **pipeline health monitoring** with Logic Apps.  
- ğŸš€ Managed **CI/CD deployments** via Azure DevOps + GitHub.  
- ğŸ“Š Built **Power BI dashboards** for insights and business reporting.  

---

## ğŸš€ Business Outcomes

- âœ… **Reliable migration** from on-prem SQL to Azure cloud.  
- âœ… **5x faster queries** using Delta Lake optimizations.  
- âœ… **Reduced storage costs** with Parquet compression.  
- âœ… **Automated monitoring** with alerts for quick issue detection.  
- âœ… **Scalable solution** supporting batch + incremental processing.  
- âœ… **Actionable insights** delivered through Power BI dashboards.  
